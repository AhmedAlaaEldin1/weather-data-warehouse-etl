from pyspark.sql import SparkSession, Window
from pyspark.sql.functions import (
    col, lit, year, month, dayofmonth, date_format,
    row_number, to_date, trim, split
)
from pyspark.sql.types import DateType
import boto3, json, os

# ====== Ø¥Ø¹Ø¯Ø§Ø¯ Spark ======
spark = SparkSession.builder \
    .appName("Weather_ETL_to_DWH_Clean") \
    .config("spark.jars", "/home/jovyan/postgresql-42.7.3.jar") \
    .getOrCreate()

# ====== Ø¥Ø¹Ø¯Ø§Ø¯ Ø§ØªØµØ§Ù„ PostgreSQL ======
jdbc_url = "jdbc:postgresql://de_postgres:5432/airflow"
properties = {
    "user": "airflow",
    "password": "airflow",
    "driver": "org.postgresql.Driver"
}

# ====== Helper: Ø¯Ø§Ù„Ø© Ù„Ù‚Ø±Ø§Ø¡Ø© JSONs Ù…Ù† MinIO (Ù…Ø«Ù„ Ù‚Ø¨Ù„) ======
def read_from_minio(bucket_name="raw-weather", endpoint="http://host.docker.internal:9000",
                    access_key="minioadmin", secret_key="minioadmin"):
    s3_client = boto3.client(
        's3',
        endpoint_url=endpoint,
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key,
        region_name='us-east-1'
    )

    # pagination
    file_keys = []
    continuation_token = None
    while True:
        if continuation_token:
            resp = s3_client.list_objects_v2(Bucket=bucket_name, ContinuationToken=continuation_token)
        else:
            resp = s3_client.list_objects_v2(Bucket=bucket_name)
        contents = resp.get("Contents", [])
        file_keys.extend([o["Key"] for o in contents])
        if resp.get("IsTruncated"):
            continuation_token = resp.get("NextContinuationToken")
        else:
            break

    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª
    records = []
    for idx, key in enumerate(file_keys, start=1):
        obj = s3_client.get_object(Bucket=bucket_name, Key=key)
        raw = obj['Body'].read().decode('utf-8')
        try:
            record = json.loads(raw)
        except Exception:
            continue
        # filename layout: raw-weather/<city>/<date>.json
        parts = key.split('/')
        if len(parts) >= 3:
            city = parts[1]
            date_str = parts[2].replace('.json', '')
            record["city"] = city
            record["date_raw"] = date_str
        records.append(record)
        if idx % 500 == 0:
            print(f"ğŸ“¥ read {idx}/{len(file_keys)} from MinIO")
    if not records:
        return None
    return spark.createDataFrame(records)

# ====== Ø£ÙˆÙ„Ø§Ù‹: Ù‡Ù„ ÙÙŠ Ù…Ù„Ù CSV Ù…Ø­Ù„ÙŠ Ø£Ø±Ø³Ù„ØªÙŠÙ‡ØŸ Ù†ÙØ¶Ù‘Ù„Ù‡ Ù„Ùˆ Ù…ÙˆØ¬ÙˆØ¯ ======
local_csv_path = "/mnt/data/weather.csv"
df_raw = None

if os.path.exists(local_csv_path):
    print(f"ğŸ“ Found local CSV at {local_csv_path} â€” will use it as primary source.")
    # Ù†Ù‚Ø±Ø£ CSV Ø§Ù„Ø£ØµÙ„ÙŠ Ù…Ø¹ infer schema (Ø£Ùˆ Ù†Ø¹ÙŠÙ‘Ù† dtypes Ø¥Ù† Ø§Ø­ØªØ§Ø¬Ù†Ø§)
    pdf = spark.read.option("header", True).csv(local_csv_path)  # ÙŠØ¹ÙˆØ¯ ÙƒÙ€ DataFrame
    # Ù†Ø¹Ø§Ù„Ø¬ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù„Ùˆ ÙÙŠÙ‡Ø§ Ù†Ù‚Ø§Ø· Ø£Ùˆ Ù…Ø³Ø§ÙØ§Øª
    # (Ø§Ù„Ù€ CSV Ø§Ù„Ø£ØµÙ„ÙŠ ÙÙŠÙ‡ Ø£Ø¹Ù…Ø¯Ø© Ù…Ø«Ù„ Data.Precipitation Ùˆ Date.Full)
    # Ù†Ø­Ø· Ø£Ø³Ù…Ø§Ø¡ Ù‚ØµÙŠØ±Ø© Ù…Ø¹Ù‚ÙˆÙ„Ø©
    col_map = {}
    for c in pdf.columns:
        new = c.strip().replace(" ", "_").replace(".", "_")
        col_map[c] = new
    for old, new in col_map.items():
        pdf = pdf.withColumnRenamed(old, new)
    # Ø§Ù„Ø¢Ù† ensure Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ù‡Ù…Ø© Ù…ÙˆØ¬ÙˆØ¯Ø©
    # Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© Ø¨Ø¹Ø¯ Ø§Ù„rename: Data_Precipitation, Date_Full, Date_Month, Date_Week_of, Date_Year,
    # Station_City, Station_Code, Station_Location, Station_State, Data_Temperature_Avg_Temp, ...
    df = pdf
    # parse date from Date_Full (format M/d/yyyy or maybe MM/dd/yyyy)
    # Ù†Ø³ØªØ®Ø¯Ù… to_date Ù…Ø¹ pattern Ù…Ø±Ù†: Ø£ÙˆÙ„ Ù†Ø­Ø§ÙˆÙ„ M/d/yyyy Ø«Ù… yyyy-MM-dd fallback
    df = df.withColumn("date_parsed", to_date(trim(col("Date_Full")), "M/d/yyyy"))
    df = df.withColumn("date_parsed", 
                       to_date(
                           when(col("date_parsed").isNull(), trim(col("Date_Full"))).otherwise(col("date_parsed")),
                           "MM/dd/yyyy"
                       ))
    # Ø¥Ø°Ø§ Ù„Ø³Ù‡ nullØŒ Ù†Ø­Ø­Ø§ÙˆÙ„ YYYY-MM-DD
    df = df.withColumn("date_parsed",
                       to_date(col("date_parsed"), "yyyy-MM-dd"))
    # rename parsed into date
    df = df.withColumnRenamed("date_parsed", "date")
    # ensure numeric columns casted
    numeric_cols = {
        "Data_Precipitation": "double",
        "Data_Temperature_Avg_Temp": "double",
        "Data_Temperature_Max_Temp": "double",
        "Data_Temperature_Min_Temp": "double",
        "Data_Wind_Speed": "double"
    }
    for src, dtype in numeric_cols.items():
        if src in df.columns:
            df = df.withColumn(src, col(src).cast("double"))
    # unify station columns
    if "Station_City" in df.columns:
        df = df.withColumnRenamed("Station_City", "station_city")
    if "Station_Code" in df.columns:
        df = df.withColumnRenamed("Station_Code", "station_code")
    if "Station_Location" in df.columns:
        df = df.withColumnRenamed("Station_Location", "station_location")
    if "Station_State" in df.columns:
        df = df.withColumnRenamed("Station_State", "station_state")
    # final expected measure column names (make them shorter)
    if "Data_Precipitation" in df.columns:
        df = df.withColumnRenamed("Data_Precipitation", "precipitation")
    if "Data_Temperature_Avg_Temp" in df.columns:
        df = df.withColumnRenamed("Data_Temperature_Avg_Temp", "avg_temp")
    if "Data_Temperature_Max_Temp" in df.columns:
        df = df.withColumnRenamed("Data_Temperature_Max_Temp", "max_temp")
    if "Data_Temperature_Min_Temp" in df.columns:
        df = df.withColumnRenamed("Data_Temperature_Min_Temp", "min_temp")
    if "Data_Wind_Speed" in df.columns:
        df = df.withColumnRenamed("Data_Wind_Speed", "wind_speed")
    # keep only rows with a valid date and city
    df = df.filter(col("date").isNotNull()).filter(col("station_city").isNotNull())
    df = df.select(
        col("station_city").alias("city"),
        col("station_code").alias("station_code") if "station_code" in df.columns else lit(None).alias("station_code"),
        col("station_location").alias("station_location") if "station_location" in df.columns else lit(None).alias("station_location"),
        col("station_state").alias("state") if "station_state" in df.columns else lit(None).alias("state"),
        col("date"),
        col("avg_temp"), col("max_temp"), col("min_temp"),
        col("precipitation"), col("wind_speed")
    )
    df_raw = df

else:
    # fallback to MinIO JSONs (previous pipeline)
    print("âš ï¸ Local CSV not found â€” falling back to MinIO JSONs.")
    df_minio = read_from_minio()
    if df_minio is None:
        raise RuntimeError("Ù„Ù… Ø£Ø¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ MinIO ÙˆÙ„Ø§ Ù…Ù„Ù CSV Ù…Ø­Ù„ÙŠ â€” Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©.")
    # MinIO JSONs likely already have avg_temp, max_temp, min_temp, precipitation, wind_speed, city, date
    # normalize column names if needed
    available = set(df_minio.columns)
    rename_map = {}
    if "avg_temp" not in available and "Data_Temperature_Avg_Temp" in available:
        rename_map["Data_Temperature_Avg_Temp"] = "avg_temp"
    # apply renames
    for old, new in rename_map.items():
        df_minio = df_minio.withColumnRenamed(old, new)
    # ensure date column named 'date'
    if "date_raw" in df_minio.columns:
        df_minio = df_minio.withColumnRenamed("date_raw", "date")
    df_minio = df_minio.withColumn("date", to_date(trim(col("date")), "yyyy-MM-dd"))
    df_raw = df_minio.select(
        col("city"),
        lit(None).alias("station_code"),
        lit(None).alias("station_location"),
        lit(None).alias("state"),
        col("date"),
        col("avg_temp"), col("max_temp"), col("min_temp"),
        col("precipitation"), col("wind_speed")
    )

# ====== Ø§Ù„Ø¢Ù† df_raw Ø¬Ø§Ù‡Ø²Ø© ÙˆÙ†Ù‚ÙˆÙ… Ø¨Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ ÙˆØ§Ù„Ù€ fact Ø¨Ø¯ÙˆÙ† Ù‚ÙŠÙ… Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø®Ø§Ø·Ø¦Ø© ======
print("â–¶ï¸ Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø®Ø§Ù… Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ:", df_raw.count())

# ====== dim_date: unique dates with proper keys ======
date_window = Window.orderBy(col("date").asc())
dim_date = df_raw.select("date").distinct() \
    .withColumn("date_id", row_number().over(date_window) - 1) \
    .withColumn("year", year("date")) \
    .withColumn("month", month("date")) \
    .withColumn("day", dayofmonth("date")) \
    .withColumn("weekday", date_format(col("date"), "E")) \
    .orderBy("date")

# ====== dim_location: unique locations (use state as region) ======
loc_window = Window.orderBy(col("city").asc())
dim_location = df_raw.select("city", "state", "station_code", "station_location") \
    .distinct() \
    .withColumn("location_id", row_number().over(loc_window) - 1) \
    .withColumn("country", lit("USA")) \
    .withColumn("region", col("state")) \
    .select("city", "location_id", "country", "region", "station_code", "station_location")

# ====== fact_weather: link to dimension keys ======
# join to get ids
fact = df_raw.join(dim_location.select("city", "location_id"), on="city", how="left") \
    .join(dim_date.select("date", "date_id"), on="date", how="left") \
    .select(
        col("location_id"),
        col("date_id"),
        col("avg_temp"),
        col("max_temp"),
        col("min_temp"),
        col("precipitation"),
        col("wind_speed")
    )

# add a deterministic surrogate key for fact (optional)
fact = fact.withColumn("weather_id", row_number().over(Window.orderBy("location_id", "date_id")) - 1)

# ====== Final simple validation (basic checks) ======
# check for nulls in FK columns
null_locs = fact.filter(col("location_id").isNull()).count()
null_dates = fact.filter(col("date_id").isNull()).count()
print(f"âš ï¸ Ø¹Ø¯Ø¯ ØµÙÙˆÙ fact Ø¨Ø¯ÙˆÙ† location_id: {null_locs}")
print(f"âš ï¸ Ø¹Ø¯Ø¯ ØµÙÙˆÙ fact Ø¨Ø¯ÙˆÙ† date_id: {null_dates}")

if null_locs > 0 or null_dates > 0:
    print("Ù‡Ù†Ø§Ùƒ ØµÙÙˆÙ Ø¨Ø¯ÙˆÙ† Ù…ÙØ§ØªÙŠØ­ Ø£Ø¨Ø¹Ø§Ø¯ â€” Ø³Ù†Ø­ÙØ¸Ù‡Ø§ Ø¥Ù„Ù‰ Ø¬Ø¯ÙˆÙ„ Ù…Ø¤Ù‚Øª Ù„Ù„ØªØ­Ù‚Ù‚ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©.")
    # save temporary and raise for inspection
    fact.write.jdbc(url=jdbc_url, table="fact_weather_temp", mode="overwrite", properties=properties)
    print("âœ… Ø­ÙØ¸Øª fact_weather_temp Ù„ÙØ­Øµ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„. Ø£ÙˆÙ‚Ù Ø§Ù„ØªÙ†ÙÙŠØ° Ø£Ùˆ Ø§ÙØ­Øµ fact_weather_temp ÙŠØ¯ÙˆÙŠØ§Ù‹.")
else:
    # ====== ØªØ­Ù…ÙŠÙ„ Ø¥Ù„Ù‰ PostgreSQL (Ù†Ø³ØªØ¨Ø¯Ù„ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ù„Ø¶Ù…Ø§Ù† ØªÙ†Ø§Ø³Ù‚) ======
    dim_date.write.jdbc(url=jdbc_url, table="dim_date", mode="overwrite", properties=properties)
    dim_location.write.jdbc(url=jdbc_url, table="dim_location", mode="overwrite", properties=properties)
    fact.write.jdbc(url=jdbc_url, table="fact_weather", mode="overwrite", properties=properties)
    print("ğŸ¯ ØªÙ… ØªØ­Ù…ÙŠÙ„ dim_date, dim_location, fact_weather Ø¥Ù„Ù‰ PostgreSQL Ø¨Ù†Ø¬Ø§Ø­!")

