# ğŸŒ¦ï¸ Weather Data Warehouse ETL Project

This project demonstrates a complete **ETL pipeline** built with **Apache Airflow**, **Apache Spark**, **PostgreSQL**, and **MinIO**.  
The goal is to extract raw weather data, transform it using PySpark, and load it into a data warehouse for analysis.

---

## ğŸš€ Project Architecture

**Tools & Technologies Used:**
- Apache Airflow (for scheduling & orchestration)
- Apache Spark (for data transformation)
- PostgreSQL (as the Data Warehouse)
- MinIO (for object storage)
- Docker & Docker Compose

---

## âš™ï¸ ETL Process

1. **Extract**  
   Weather data is stored in `weather.csv` and uploaded to MinIO using `Dag1.py`.

2. **Transform**  
   The Spark script (`ETL_SCRIPT_pyspark.py`) cleans, transforms, and prepares the data.

3. **Load**  
   The processed data is written into PostgreSQL tables, forming the data warehouse.

---

## ğŸ—‚ï¸ Project Structure

```
weather-data-warehouse-etl/
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ Dag1.py
â”‚   â”œâ”€â”€ Dag2.py
â”‚   â”œâ”€â”€ Dagtest2.py
â”‚
â”œâ”€â”€ spark_scripts/
â”‚   â”œâ”€â”€ ETL_SCRIPT_pyspark.py
â”‚   â”œâ”€â”€ SQL-file_pyspark.py
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile.airflow
â”‚   â”œâ”€â”€ Dockerfile.spark
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ weather.csv
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§© How to Run

```bash
# 1ï¸âƒ£ Build and start all containers
docker-compose up -d

# 2ï¸âƒ£ Access Airflow UI
http://localhost:8080

# 3ï¸âƒ£ Trigger the DAGs from the Airflow web interface

# 4ï¸âƒ£ View transformed data in PostgreSQL
```

---

## ğŸ‘¤ Author

**Ahmed Alaa Eldin**  
Data Engineer | ETL Developer  
[GitHub Profile](https://github.com/AhmedAlaaEldin1)
